{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL4a Demos of machine learning models with the iris dataset\n",
    "**Data Set Information:**\n",
    "This is perhaps the best known database to be found in the pattern recognition literature. Fisher's paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.\n",
    "\n",
    "**Predicted attribute:** class of iris plant.\n",
    "This is an exceedingly simple domain.\n",
    "This data differs from the data presented in Fishers article (identified by Steve Chadwick, spchadwick '@' espeedaz.net ). The 35th sample should be: 4.9,3.1,1.5,0.2,\"Iris-setosa\" where the error is in the fourth feature. The 38th sample: 4.9,3.6,1.4,0.1,\"Iris-setosa\" where the errors are in the second and third features.\n",
    "\n",
    "**Attribute Information:**\n",
    "1. sepal length in cm\n",
    "2. sepal width in cm\n",
    "3. petal length in cm\n",
    "4. petal width in cm\n",
    "5. class:\n",
    "-- Iris Setosa\n",
    "-- Iris Versicolour\n",
    "-- Iris Virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import time\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :4]  # we only take the first two features.\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.333)\n",
    "#print(len(X_train), len(X_test))\n",
    "def print_accuracy(f):\n",
    "    print(\"Accuracy = {0}%\".format(100*np.sum(f(X_test) == y_test)/len(y_test)))\n",
    "    time.sleep(0.5) # to let the print get out before any progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN: k-nearest neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "print_accuracy(knn.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM: Support vector machine with a linear kernel\n",
    "from sklearn.svm import SVC\n",
    "svc_linear = SVC(kernel='linear', probability=True)\n",
    "svc_linear.fit(X_train, y_train)\n",
    "print_accuracy(svc_linear.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DT: Decision tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "linear_lr = LogisticRegression()\n",
    "linear_lr.fit(X_train, y_train)\n",
    "print_accuracy(linear_lr.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF: Random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rforest = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=2, random_state=0)\n",
    "rforest.fit(X_train, y_train)\n",
    "print_accuracy(rforest.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN: Neural network (MLP)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "nn = MLPClassifier(solver='lbfgs', alpha=1e-1, hidden_layer_sizes=(5, 2), random_state=0)\n",
    "nn.fit(X_train, y_train)\n",
    "print_accuracy(nn.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an XGB classifier and instance of the same\n",
    "from sklearn import svm\n",
    "from xgboost import XGBClassifier\n",
    "clf = XGBClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print_accuracy(clf.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression 3-class Classifier\n",
    "https://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=1e5)\n",
    "\n",
    "# Create an instance of Logistic Regression Classifier and fit the data.\n",
    "logreg.fit(X_train, y_train)\n",
    "print_accuracy(logreg.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
